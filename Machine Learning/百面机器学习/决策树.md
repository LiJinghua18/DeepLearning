# 决策树

### 决策树基本知识

**定义：**

​		一种自上而下，岁样本数据进行树形分类的过程，由结点和有向边组成。

**结点**：叶节点——分类结果，类别

​           内部结点——表示特征或属性

应用于集成学习得到——随机森林、梯度提升决策树等

**决策树生成过程：**

​		特征选择、树的构造、树的剪枝

------

**问题描述**：决策树有哪些常用的启发函数？

ID3（最大信息增益）,C4.5(最大信息增益比),CART(Classification and Regression Tree，分类回归树，最大基尼指数)

**三者区别：**

（1）ID3采用信息增益作为评价标准，倾向于取值较多的特征，C4.5对其进行优化，对取值较多的特征进行惩罚，避免ID3出现的过拟合，提升决策树的泛化能力

（2）**样本类型**：ID3只能处理离散型变量，而C4.5和CART都可以处理连续变量

（3）**应用角度：**ID3和C4.5只能用于分类，CART既可以分类也可以回归。

（4）**实现细节、优化过程：**

​			①ID3对样本缺失值比较敏感，而C4.5和CART可以对缺失值进行不同方式的处理；

​			②ID3和C4.5每个结点可以产生多个分支，且每个特征在层级之间不会复用，而CART每个结点只有两个分支，相当于二叉树，特征可以被复用

​			③ID3、C4.5通过剪枝权衡树的准确性与泛化能力，而CART直接利用全部数据发现所有可能的树结构进行对比。

------

##### **问题描述**：如何对决策树进行剪枝？

###### 预剪枝

**核心思想：**

​		树中结点扩展之前，先计算当前的划分是否能提升模型的泛化能力，若不能，则停止扩展子树。最后按照多数投票原则判断该节点所属类别。

**判断停止决策树生长的方法：**

（1）树达到一定深度时

（2）当前结点样本数小于某个阈值时

（3）每次分裂对测试集的准确度提升小于某个阈值时



###### **后剪枝：**

**核心思想：**让算法生成一棵完全生长的决策树，然后从最底层向上计算是否剪枝。剪枝构成将子树删除，用一个叶子结点替代，该节点类别也按照多数投票原则进行判断。

**特点：**泛化能力更强，但时间开销更大。

**方法：**

错误率降低剪枝、悲观剪枝、代价复杂度剪枝、最小误差剪枝、CVP、OPP。

